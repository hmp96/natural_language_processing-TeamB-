{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1D6B6kbsGGQJY5vHEumccsoBFCiw12bUg","timestamp":1670547288579}],"mount_file_id":"1D6B6kbsGGQJY5vHEumccsoBFCiw12bUg","authorship_tag":"ABX9TyOBCZkUTJ4/h3iJaGm4g6eF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-R4v047M1Xft","executionInfo":{"status":"ok","timestamp":1670471083897,"user_tz":-540,"elapsed":10670,"user":{"displayName":"_잎새","userId":"11242500208635470628"}},"outputId":"7729ff4b-ca2d-40d6-c84d-66b900bb5d75"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive"],"metadata":{"id":"FCs919dQrtIY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ls"],"metadata":{"id":"ppNJAYpfrt33"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd"],"metadata":{"id":"YBvjG0V21fVm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["movies=pd.read_csv('movies_genres.csv')"],"metadata":{"id":"br6Ij4rMR4lj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data=pd.read_excel('감성대화말뭉치(최종데이터)_Training.xlsx')"],"metadata":{"id":"TJqWL1GKR49j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data=data.iloc[:,5:8]"],"metadata":{"id":"rA7M4lSaR5AB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data.drop('감정_소분류', axis=1, inplace=True)"],"metadata":{"id":"wBtuiJShR5CE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data.columns=['감정', '사람']\n","# data"],"metadata":{"id":"YjDPt45zR5Ei"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"tO7akc1FR5GU","executionInfo":{"status":"ok","timestamp":1670473909988,"user_tz":-540,"elapsed":77,"user":{"displayName":"_잎새","userId":"11242500208635470628"}},"outputId":"73ff6786-340e-41df-b6a9-fabd9f24de10"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   감정                                             사람\n","0  기쁨                      아내가 드디어 출산하게 되어서 정말 신이 나.\n","1  불안        당뇨랑 합병증 때문에 먹어야 할 약이 열 가지가 넘어가니까 스트레스야.\n","2  당황        고등학교에 올라오니 중학교 때보다 수업이 갑자기 어려워져서 당황스러워.\n","3  기쁨  재취업이 돼서 받게 된 첫 월급으로 온 가족이 외식을 할 예정이야. 너무 행복해.\n","4  기쁨                   빚을 드디어 다 갚게 되어서 이제야 안도감이 들어."],"text/html":["\n","  <div id=\"df-f07764f6-bb0e-4c50-8728-7bbe518cbb09\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>감정</th>\n","      <th>사람</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>기쁨</td>\n","      <td>아내가 드디어 출산하게 되어서 정말 신이 나.</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>불안</td>\n","      <td>당뇨랑 합병증 때문에 먹어야 할 약이 열 가지가 넘어가니까 스트레스야.</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>당황</td>\n","      <td>고등학교에 올라오니 중학교 때보다 수업이 갑자기 어려워져서 당황스러워.</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>기쁨</td>\n","      <td>재취업이 돼서 받게 된 첫 월급으로 온 가족이 외식을 할 예정이야. 너무 행복해.</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>기쁨</td>\n","      <td>빚을 드디어 다 갚게 되어서 이제야 안도감이 들어.</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f07764f6-bb0e-4c50-8728-7bbe518cbb09')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-f07764f6-bb0e-4c50-8728-7bbe518cbb09 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-f07764f6-bb0e-4c50-8728-7bbe518cbb09');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":66}]},{"cell_type":"code","source":["data=data.replace('불안', 0).replace('분노', 1).replace('슬픔', 2).replace('기쁨', 3).replace('상처', 4).replace('당황', 5)"],"metadata":{"id":"hXYH6rBCR5Is"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# data.감정.unique()\n","data=data.replace('불안 ', 0).replace('기쁨 ', 1)"],"metadata":{"id":"jgVtcLkNTePX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data.감정.unique()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HrbeUwMDTd4E","executionInfo":{"status":"ok","timestamp":1670473910000,"user_tz":-540,"elapsed":82,"user":{"displayName":"_잎새","userId":"11242500208635470628"}},"outputId":"2f3b736a-f2bd-4576-f30a-b566d6e00946"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([3, 0, 5, 2, 1, 4])"]},"metadata":{},"execution_count":69}]},{"cell_type":"code","source":["data.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"GcV22B-p8GEg","executionInfo":{"status":"ok","timestamp":1670473910005,"user_tz":-540,"elapsed":72,"user":{"displayName":"_잎새","userId":"11242500208635470628"}},"outputId":"56662719-98ef-43eb-91eb-d813edc6b397"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   감정                                             사람\n","0   3                      아내가 드디어 출산하게 되어서 정말 신이 나.\n","1   0        당뇨랑 합병증 때문에 먹어야 할 약이 열 가지가 넘어가니까 스트레스야.\n","2   5        고등학교에 올라오니 중학교 때보다 수업이 갑자기 어려워져서 당황스러워.\n","3   3  재취업이 돼서 받게 된 첫 월급으로 온 가족이 외식을 할 예정이야. 너무 행복해.\n","4   3                   빚을 드디어 다 갚게 되어서 이제야 안도감이 들어."],"text/html":["\n","  <div id=\"df-c4d37de0-5d44-4f09-9657-448f795dc3d0\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>감정</th>\n","      <th>사람</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>3</td>\n","      <td>아내가 드디어 출산하게 되어서 정말 신이 나.</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>당뇨랑 합병증 때문에 먹어야 할 약이 열 가지가 넘어가니까 스트레스야.</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>5</td>\n","      <td>고등학교에 올라오니 중학교 때보다 수업이 갑자기 어려워져서 당황스러워.</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>재취업이 돼서 받게 된 첫 월급으로 온 가족이 외식을 할 예정이야. 너무 행복해.</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>3</td>\n","      <td>빚을 드디어 다 갚게 되어서 이제야 안도감이 들어.</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c4d37de0-5d44-4f09-9657-448f795dc3d0')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-c4d37de0-5d44-4f09-9657-448f795dc3d0 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-c4d37de0-5d44-4f09-9657-448f795dc3d0');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":70}]},{"cell_type":"code","source":[],"metadata":{"id":"RsL4s7zeV1B9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install mxnet\n","!pip install gluonnlp\n","!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-b2t-fJauF-z","executionInfo":{"status":"ok","timestamp":1670471418536,"user_tz":-540,"elapsed":150231,"user":{"displayName":"_잎새","userId":"11242500208635470628"}},"outputId":"e43bab4f-1118-4b7a-d19b-e217e7662019"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting mxnet\n","  Downloading mxnet-1.9.1-py3-none-manylinux2014_x86_64.whl (49.1 MB)\n","\u001b[K     |████████████████████████████████| 49.1 MB 1.2 MB/s \n","\u001b[?25hRequirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.8/dist-packages (from mxnet) (1.21.6)\n","Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.8/dist-packages (from mxnet) (2.23.0)\n","Collecting graphviz<0.9.0,>=0.8.1\n","  Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.20.0->mxnet) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.20.0->mxnet) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.20.0->mxnet) (2022.9.24)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.20.0->mxnet) (1.24.3)\n","Installing collected packages: graphviz, mxnet\n","  Attempting uninstall: graphviz\n","    Found existing installation: graphviz 0.10.1\n","    Uninstalling graphviz-0.10.1:\n","      Successfully uninstalled graphviz-0.10.1\n","Successfully installed graphviz-0.8.4 mxnet-1.9.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting gluonnlp\n","  Downloading gluonnlp-0.10.0.tar.gz (344 kB)\n","\u001b[K     |████████████████████████████████| 344 kB 31.7 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.8/dist-packages (from gluonnlp) (1.21.6)\n","Requirement already satisfied: cython in /usr/local/lib/python3.8/dist-packages (from gluonnlp) (0.29.32)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from gluonnlp) (21.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->gluonnlp) (3.0.9)\n","Building wheels for collected packages: gluonnlp\n","  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gluonnlp: filename=gluonnlp-0.10.0-cp38-cp38-linux_x86_64.whl size=619633 sha256=87b6b3b6cd018d33bed7ae0ad6da529cb4e6c128a47eef3dc13ef6005c0a9113\n","  Stored in directory: /root/.cache/pip/wheels/b6/93/9d/2237550c409eb3ed725d6302b7897ddd9a037b40cef66dcd9c\n","Successfully built gluonnlp\n","Installing collected packages: gluonnlp\n","Successfully installed gluonnlp-0.10.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://****@github.com/SKTBrain/KoBERT.git@master\n","  Cloning https://****@github.com/SKTBrain/KoBERT.git (to revision master) to /tmp/pip-req-build-wceldnob\n","  Running command git clone -q 'https://****@github.com/SKTBrain/KoBERT.git' /tmp/pip-req-build-wceldnob\n","Collecting boto3<=1.15.18\n","  Downloading boto3-1.15.18-py2.py3-none-any.whl (129 kB)\n","\u001b[K     |████████████████████████████████| 129 kB 39.6 MB/s \n","\u001b[?25hRequirement already satisfied: gluonnlp<=0.10.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from kobert==0.2.3) (0.10.0)\n","Collecting mxnet<=1.7.0.post2,>=1.4.0\n","  Downloading mxnet-1.7.0.post2-py2.py3-none-manylinux2014_x86_64.whl (54.7 MB)\n","\u001b[K     |████████████████████████████████| 54.7 MB 20 kB/s \n","\u001b[?25hCollecting onnxruntime<=1.8.0,==1.8.0\n","  Downloading onnxruntime-1.8.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n","\u001b[K     |████████████████████████████████| 4.5 MB 53.5 MB/s \n","\u001b[?25hCollecting sentencepiece<=0.1.96,>=0.1.6\n","  Downloading sentencepiece-0.1.96-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 62.4 MB/s \n","\u001b[?25hCollecting torch<=1.10.1,>=1.7.0\n","  Downloading torch-1.10.1-cp38-cp38-manylinux1_x86_64.whl (881.9 MB)\n","\u001b[K     |██████████████████████████████▎ | 834.1 MB 1.2 MB/s eta 0:00:40tcmalloc: large alloc 1147494400 bytes == 0x39496000 @  0x7f43bf878615 0x5d631c 0x51e4f1 0x51e67b 0x4f7585 0x49ca7c 0x4fdff5 0x49caa1 0x4fdff5 0x49ced5 0x4f60a9 0x55f926 0x4f60a9 0x55f926 0x4f60a9 0x55f926 0x5d7c18 0x5d9412 0x586636 0x5d813c 0x55f3fd 0x55e571 0x5d7cf1 0x49ced5 0x55e571 0x5d7cf1 0x49ec69 0x5d7c18 0x49ca7c 0x4fdff5 0x49ced5\n","\u001b[K     |████████████████████████████████| 881.9 MB 8.1 kB/s \n","\u001b[?25hCollecting transformers<=4.8.1,>=4.8.1\n","  Downloading transformers-4.8.1-py3-none-any.whl (2.5 MB)\n","\u001b[K     |████████████████████████████████| 2.5 MB 52.1 MB/s \n","\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.8/dist-packages (from onnxruntime<=1.8.0,==1.8.0->kobert==0.2.3) (3.19.6)\n","Requirement already satisfied: flatbuffers in /usr/local/lib/python3.8/dist-packages (from onnxruntime<=1.8.0,==1.8.0->kobert==0.2.3) (1.12)\n","Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.8/dist-packages (from onnxruntime<=1.8.0,==1.8.0->kobert==0.2.3) (1.21.6)\n","Collecting botocore<1.19.0,>=1.18.18\n","  Downloading botocore-1.18.18-py2.py3-none-any.whl (6.7 MB)\n","\u001b[K     |████████████████████████████████| 6.7 MB 26.8 MB/s \n","\u001b[?25hCollecting s3transfer<0.4.0,>=0.3.0\n","  Downloading s3transfer-0.3.7-py2.py3-none-any.whl (73 kB)\n","\u001b[K     |████████████████████████████████| 73 kB 2.7 MB/s \n","\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n","  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n","Requirement already satisfied: urllib3<1.26,>=1.20 in /usr/local/lib/python3.8/dist-packages (from botocore<1.19.0,>=1.18.18->boto3<=1.15.18->kobert==0.2.3) (1.24.3)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.8/dist-packages (from botocore<1.19.0,>=1.18.18->boto3<=1.15.18->kobert==0.2.3) (2.8.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from gluonnlp<=0.10.0,>=0.6.0->kobert==0.2.3) (21.3)\n","Requirement already satisfied: cython in /usr/local/lib/python3.8/dist-packages (from gluonnlp<=0.10.0,>=0.6.0->kobert==0.2.3) (0.29.32)\n","Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.8/dist-packages (from mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (0.8.4)\n","Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.8/dist-packages (from mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (2.23.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.19.0,>=1.18.18->boto3<=1.15.18->kobert==0.2.3) (1.15.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.20.0->mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.20.0->mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.20.0->mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (2022.9.24)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch<=1.10.1,>=1.7.0->kobert==0.2.3) (4.4.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (4.64.1)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (6.0)\n","Collecting huggingface-hub==0.0.12\n","  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (3.8.0)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n","\u001b[K     |████████████████████████████████| 880 kB 64.9 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (2022.6.2)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 52.0 MB/s \n","\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->gluonnlp<=0.10.0,>=0.6.0->kobert==0.2.3) (3.0.9)\n","Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (1.2.0)\n","Building wheels for collected packages: kobert, sacremoses\n","  Building wheel for kobert (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for kobert: filename=kobert-0.2.3-py3-none-any.whl size=15708 sha256=2561b851bca591b5c6cc8663b95edd51ec8f860e215c4a30edc2925314242a76\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-hjbpulto/wheels/bf/5f/74/81bf3a1332130eb6629ecf58876a8746b77021e7d7b0638e91\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=922e545ab4b6833a0b803759ed718ddc78a191b5418b42e25c501fad4a6e9212\n","  Stored in directory: /root/.cache/pip/wheels/82/ab/9b/c15899bf659ba74f623ac776e861cf2eb8608c1825ddec66a4\n","Successfully built kobert sacremoses\n","Installing collected packages: jmespath, botocore, tokenizers, sacremoses, s3transfer, huggingface-hub, transformers, torch, sentencepiece, onnxruntime, mxnet, boto3, kobert\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.13.0+cu116\n","    Uninstalling torch-1.13.0+cu116:\n","      Successfully uninstalled torch-1.13.0+cu116\n","  Attempting uninstall: mxnet\n","    Found existing installation: mxnet 1.9.1\n","    Uninstalling mxnet-1.9.1:\n","      Successfully uninstalled mxnet-1.9.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchvision 0.14.0+cu116 requires torch==1.13.0, but you have torch 1.10.1 which is incompatible.\n","torchtext 0.14.0 requires torch==1.13.0, but you have torch 1.10.1 which is incompatible.\n","torchaudio 0.13.0+cu116 requires torch==1.13.0, but you have torch 1.10.1 which is incompatible.\u001b[0m\n","Successfully installed boto3-1.15.18 botocore-1.18.18 huggingface-hub-0.0.12 jmespath-0.10.0 kobert-0.2.3 mxnet-1.7.0.post2 onnxruntime-1.8.0 s3transfer-0.3.7 sacremoses-0.0.53 sentencepiece-0.1.96 tokenizers-0.10.3 torch-1.10.1 transformers-4.8.1\n"]}]},{"cell_type":"code","source":["import torch\n","from torch import nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import gluonnlp as nlp\n","import numpy as np\n","from tqdm import tqdm"],"metadata":{"id":"e1_ZUcgDuGAl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#kobert\n","from kobert.utils import get_tokenizer\n","from kobert.pytorch_kobert import get_pytorch_kobert_model\n","\n","#transformers\n","from transformers import AdamW\n","from transformers.optimization import get_cosine_schedule_with_warmup"],"metadata":{"id":"nR8V6jJ2uGCe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["max_len = 64   # 텍스트 데이터 최대 길이\n","batch_size = 64\n","warmup_ratio = 0.1\n","num_epochs = 5\n","max_grad_norm = 1\n","log_interval = 200\n","learning_rate =  5e-5\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"metadata":{"id":"8FcgrEycuGET"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_list = []\n","for ques, label in zip(data['사람'], data['감정'])  :\n","    data = []   \n","    data.append(ques)\n","    data.append(str(label))\n","    data_list.append(data)"],"metadata":{"id":"VJ7B9uYMuGGe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","dataset_train, dataset_test = train_test_split(data_list, test_size=0.2, shuffle=True, random_state=34)"],"metadata":{"id":"0IRslE7YuGIM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class BERTDataset(Dataset):\n","    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n","                 pad, pair):\n","        transform = nlp.data.BERTSentenceTransform(\n","            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n","\n","        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n","        self.labels = [np.int32(i[label_idx]) for i in dataset]\n","\n","    def __getitem__(self, i):\n","        return (self.sentences[i] + (self.labels[i], ))\n","\n","    def __len__(self):\n","        return (len(self.labels))"],"metadata":{"id":"FWCNCY4jtXqo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bertmodel, vocab = get_pytorch_kobert_model()\n","\n","#토큰화\n","tokenizer = get_tokenizer()\n","tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n","\n","data_train = BERTDataset(dataset_train, 0, 1, tok, max_len, True, False)\n","data_test = BERTDataset(dataset_test, 0, 1, tok, max_len, True, False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dREeMQ-wtXsj","executionInfo":{"status":"ok","timestamp":1670462726694,"user_tz":-540,"elapsed":6767,"user":{"displayName":"_잎새","userId":"11242500208635470628"}},"outputId":"0497daee-fc3c-4866-be69-10922a504ed5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["using cached model. /content/.cache/kobert_v1.zip\n","using cached model. /content/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n","using cached model. /content/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n"]}]},{"cell_type":"code","source":["train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5)\n","test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D5KRGeb8tXuw","executionInfo":{"status":"ok","timestamp":1670462726696,"user_tz":-540,"elapsed":60,"user":{"displayName":"_잎새","userId":"11242500208635470628"}},"outputId":"df3b2319-bc2e-4930-9b96-66779ff79e43"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]}]},{"cell_type":"code","source":["class BERTClassifier(nn.Module):\n","    def __init__(self,\n","                 bert,\n","                 hidden_size = 768,\n","                 num_classes=6, \n","                 dr_rate=None,\n","                 params=None):\n","        super(BERTClassifier, self).__init__()\n","        self.bert = bert\n","        self.dr_rate = dr_rate\n","                 \n","        self.classifier = nn.Linear(hidden_size , num_classes)\n","        if dr_rate:\n","            self.dropout = nn.Dropout(p=dr_rate)\n","    \n","    def gen_attention_mask(self, token_ids, valid_length):\n","        attention_mask = torch.zeros_like(token_ids)\n","        for i, v in enumerate(valid_length):\n","            attention_mask[i][:v] = 1\n","        return attention_mask.float()\n","\n","    def forward(self, token_ids, valid_length, segment_ids):\n","        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n","        \n","        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(),\n","                              attention_mask = attention_mask.float().to(token_ids.device),return_dict=False)\n","        if self.dr_rate:\n","            out = self.dropout(pooler)\n","        return self.classifier(out)"],"metadata":{"id":"ngssfYsLtXwx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = BERTClassifier(bertmodel,  dr_rate=0.5).to(device)"],"metadata":{"id":"xAZs1Jy3tXyp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["no_decay = ['bias', 'LayerNorm.weight']\n","optimizer_grouped_parameters = [\n","    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","]\n","\n","optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n","loss_fn = nn.CrossEntropyLoss() # 다중분류를 위한 대표적인 loss func\n","\n","t_total = len(train_dataloader) * num_epochs\n","warmup_step = int(t_total * warmup_ratio)\n","\n","scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)"],"metadata":{"id":"6_3eUMMhtX01"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def calc_accuracy(X,Y):\n","    max_vals, max_indices = torch.max(X, 1)\n","    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n","    return train_acc\n","    \n","train_dataloader"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L86T89gGtX2s","executionInfo":{"status":"ok","timestamp":1670462730412,"user_tz":-540,"elapsed":31,"user":{"displayName":"_잎새","userId":"11242500208635470628"}},"outputId":"efdad4de-ec4e-4d62-b537-fe01fe0484ad"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch.utils.data.dataloader.DataLoader at 0x7f8fe6e11190>"]},"metadata":{},"execution_count":58}]},{"cell_type":"code","source":["for e in range(num_epochs):\n","    train_acc = 0.0\n","    test_acc = 0.0\n","    model.train()\n","    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(train_dataloader)):\n","        optimizer.zero_grad()\n","        token_ids = token_ids.long().to(device)\n","        segment_ids = segment_ids.long().to(device)\n","        valid_length= valid_length\n","        label = label.long().to(device)\n","        out = model(token_ids, valid_length, segment_ids)\n","        loss = loss_fn(out, label)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n","        optimizer.step()\n","        scheduler.step()  # Update learning rate schedule\n","        train_acc += calc_accuracy(out, label)\n","        if batch_id % log_interval == 0:\n","            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n","    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n","    \n","    model.eval()\n","    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(test_dataloader)):\n","        token_ids = token_ids.long().to(device)\n","        segment_ids = segment_ids.long().to(device)\n","        valid_length= valid_length\n","        label = label.long().to(device)\n","        out = model(token_ids, valid_length, segment_ids)\n","        test_acc += calc_accuracy(out, label)\n","    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6ficG31MvHuG","executionInfo":{"status":"ok","timestamp":1670464741755,"user_tz":-540,"elapsed":1789465,"user":{"displayName":"_잎새","userId":"11242500208635470628"}},"outputId":"0618aadc-7b37-499f-bc71-b97bdf6a6739"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/511 [00:00<?, ?it/s]/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","  0%|          | 1/511 [00:01<08:33,  1.01s/it]"]},{"output_type":"stream","name":"stdout","text":["epoch 1 batch id 1 loss 1.708181381225586 train acc 0.25\n"]},{"output_type":"stream","name":"stderr","text":[" 39%|███▉      | 201/511 [02:08<03:17,  1.57it/s]"]},{"output_type":"stream","name":"stdout","text":["epoch 1 batch id 201 loss 1.1568039655685425 train acc 0.48880597014925375\n"]},{"output_type":"stream","name":"stderr","text":[" 78%|███████▊  | 401/511 [04:16<01:11,  1.54it/s]"]},{"output_type":"stream","name":"stdout","text":["epoch 1 batch id 401 loss 1.063295602798462 train acc 0.5496804862842892\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 511/511 [05:27<00:00,  1.56it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch 1 train acc 0.5635624398161091\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 128/128 [00:28<00:00,  4.48it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch 1 test acc 0.6284993489583334\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 1/511 [00:01<09:36,  1.13s/it]"]},{"output_type":"stream","name":"stdout","text":["epoch 2 batch id 1 loss 0.977314829826355 train acc 0.640625\n"]},{"output_type":"stream","name":"stderr","text":[" 39%|███▉      | 201/511 [02:10<03:19,  1.56it/s]"]},{"output_type":"stream","name":"stdout","text":["epoch 2 batch id 201 loss 1.083940029144287 train acc 0.6363495024875622\n"]},{"output_type":"stream","name":"stderr","text":[" 78%|███████▊  | 401/511 [04:19<01:11,  1.54it/s]"]},{"output_type":"stream","name":"stdout","text":["epoch 2 batch id 401 loss 0.8385356664657593 train acc 0.6513793640897756\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 511/511 [05:30<00:00,  1.55it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch 2 train acc 0.6569464829310719\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 128/128 [00:28<00:00,  4.48it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch 2 test acc 0.6438395182291666\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 1/511 [00:00<08:23,  1.01it/s]"]},{"output_type":"stream","name":"stdout","text":["epoch 3 batch id 1 loss 0.9155349731445312 train acc 0.640625\n"]},{"output_type":"stream","name":"stderr","text":[" 39%|███▉      | 201/511 [02:09<03:19,  1.56it/s]"]},{"output_type":"stream","name":"stdout","text":["epoch 3 batch id 201 loss 0.8766100406646729 train acc 0.6943407960199005\n"]},{"output_type":"stream","name":"stderr","text":[" 78%|███████▊  | 401/511 [04:18<01:10,  1.55it/s]"]},{"output_type":"stream","name":"stdout","text":["epoch 3 batch id 401 loss 0.6761960387229919 train acc 0.7123207605985037\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 511/511 [05:29<00:00,  1.55it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch 3 train acc 0.7195100642996924\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 128/128 [00:28<00:00,  4.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch 3 test acc 0.6512044270833334\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 1/511 [00:00<08:13,  1.03it/s]"]},{"output_type":"stream","name":"stdout","text":["epoch 4 batch id 1 loss 0.7667913436889648 train acc 0.71875\n"]},{"output_type":"stream","name":"stderr","text":[" 39%|███▉      | 201/511 [02:09<03:19,  1.56it/s]"]},{"output_type":"stream","name":"stdout","text":["epoch 4 batch id 201 loss 0.7723081707954407 train acc 0.7517879353233831\n"]},{"output_type":"stream","name":"stderr","text":[" 78%|███████▊  | 401/511 [04:18<01:11,  1.54it/s]"]},{"output_type":"stream","name":"stdout","text":["epoch 4 batch id 401 loss 0.549115777015686 train acc 0.7678849750623441\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 511/511 [05:29<00:00,  1.55it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch 4 train acc 0.7738769880098157\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 128/128 [00:28<00:00,  4.49it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch 4 test acc 0.6484781901041666\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 1/511 [00:01<09:26,  1.11s/it]"]},{"output_type":"stream","name":"stdout","text":["epoch 5 batch id 1 loss 0.7061077952384949 train acc 0.796875\n"]},{"output_type":"stream","name":"stderr","text":[" 39%|███▉      | 201/511 [02:09<03:20,  1.54it/s]"]},{"output_type":"stream","name":"stdout","text":["epoch 5 batch id 201 loss 0.6705185174942017 train acc 0.7951648009950248\n"]},{"output_type":"stream","name":"stderr","text":[" 78%|███████▊  | 401/511 [04:18<01:10,  1.55it/s]"]},{"output_type":"stream","name":"stdout","text":["epoch 5 batch id 401 loss 0.4694223999977112 train acc 0.8040445760598504\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 511/511 [05:29<00:00,  1.55it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch 5 train acc 0.8069325047370547\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 128/128 [00:28<00:00,  4.51it/s]"]},{"output_type":"stream","name":"stdout","text":["epoch 5 test acc 0.6515706380208334\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["def predict(predict_sentence):\n","\n","    data = [predict_sentence, '0']\n","    dataset_another = [data]\n","\n","    another_test = BERTDataset(dataset_another, 0, 1, tok, max_len, True, False)\n","    test_dataloader = torch.utils.data.DataLoader(another_test, batch_size=batch_size, num_workers=6)\n","    \n","    model.eval()\n","\n","    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n","        token_ids = token_ids.long().to(device)\n","        segment_ids = segment_ids.long().to(device)\n","\n","        valid_length= valid_length\n","        label = label.long().to(device)\n","\n","        out = model(token_ids, valid_length, segment_ids)\n","\n","        test_eval=[]\n","        re_movie=[]\n","        for i in out:\n","            logits=i\n","            logits = logits.detach().cpu().numpy()\n","\n","            if np.argmax(logits) == 0:\n","                test_eval.append(\"불안한\")\n","                re_movie.append(movies.제목[movies['영화'] == np.argmax(logits)].to_list())\n","            elif np.argmax(logits) == 1:\n","                test_eval.append(\"분노가 치미는\")\n","                re_movie.append(movies.제목[movies['영화'] == np.argmax(logits)].to_list())\n","            elif np.argmax(logits) == 2:\n","                test_eval.append(\"슬픈\")\n","                re_movie.append(movies.제목[movies['영화'] == np.argmax(logits)].to_list())\n","            elif np.argmax(logits) == 3:\n","                test_eval.append(\"기쁜\")\n","                re_movie.append(movies.제목[movies['영화'] == np.argmax(logits)].to_list())\n","            elif np.argmax(logits) == 4:\n","                test_eval.append(\"당황스러운\")\n","                re_movie.append(movies.제목[movies['영화'] == np.argmax(logits)].to_list())\n","            elif np.argmax(logits) == 5:\n","                test_eval.append(\"상처 받는\")\n","                re_movie.append(movies.제목[movies['영화'] == np.argmax(logits)].to_list())\n","\n","        print(\"오늘 \" + test_eval[0] + \" 일이 있었군요\")\n","        \n","        ans=input(\"오늘 기분에 맞는 영화를 추천해드릴까요? \")\n","        \n","        if ans == '응' :\n","          print(\"오늘 추천 드릴 영화는 \" + re_movie[0][0] +\", \" +re_movie[0][1] +\", \" +re_movie[0][2]  + \" 입니다.\")\n","        else :\n","          print(\"좋은 하루 되세요\")\n"],"metadata":{"id":"mEYqjU0HvHwI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["end = 1\n","while end == 1 :\n","    sentence = input(\"오늘 있었던 일을 입력해주세요 : \")\n","    if sentence == \"그만\" :\n","        break\n","    predict(sentence)\n","    print(\"\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sKh1mAnBvHyT","executionInfo":{"status":"ok","timestamp":1670465300386,"user_tz":-540,"elapsed":55689,"user":{"displayName":"_잎새","userId":"11242500208635470628"}},"outputId":"8450680b-bf34-4cc3-e582-9cec887e3951"},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["오늘 있었던 일을 입력해주세요 : 오늘 친구랑 맛있는 음식을 먹었어\n","오늘 기쁜 일이 있었군요\n","오늘 기분에 맞는 영화를 추천해드릴까요? 응\n","오늘 추천 드릴 영화는 스파이 지니어스, 온워드: 단 하루의 기적, 겨울왕국 2 입니다.\n","\n","\n","오늘 있었던 일을 입력해주세요 : 어제 밤에 지인이랑 싸웠어\n","오늘 당황스러운 일이 있었군요\n","오늘 기분에 맞는 영화를 추천해드릴까요? 아니\n","좋은 하루 되세요\n","\n","\n","오늘 있었던 일을 입력해주세요 : 밤새 코드 짠 거를 저장도 못하고 날렸어\n","오늘 분노가 치미는 일이 있었군요\n","오늘 기분에 맞는 영화를 추천해드릴까요? 응\n","오늘 추천 드릴 영화는 반도, 테넷, 미드웨이 입니다.\n","\n","\n","오늘 있었던 일을 입력해주세요 : 그만\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"26hMQWXc8r22"},"execution_count":null,"outputs":[]}]}